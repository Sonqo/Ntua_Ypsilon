{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hLFXD3IePSyL"
   },
   "source": [
    "## Στοιχεία Ομάδας\n",
    "\n",
    "Αριθμός Ομάδας: Μ.Α.11\n",
    "\n",
    "    > Καρυπιάδης Ευστράτιος: 03114911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UTNgwBfjPSyc"
   },
   "source": [
    "## 1. Υλοποίηση συστήματος συστάσεων ταινιών βασισμένο στο περιεχόμενο\n",
    "<img src=\"http://clture.org/wp-content/uploads/2015/12/Netflix-Streaming-End-of-Year-Posts.jpg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aViHqlQcPSyP"
   },
   "source": [
    "### Εισαγωγή του Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZVmdDExPSyQ"
   },
   "source": [
    "Το σύνολο δεδομένων με το οποίο θα δουλέψουμε είναι βασισμένο στο [Carnegie Mellon Movie Summary Corpus](http://www.cs.cmu.edu/~ark/personas/). Πρόκειται για ένα dataset με περίπου 40.000 περιγραφές ταινιών. Η περιγραφή κάθε ταινίας αποτελείται από τον τίτλο της, μια ή περισσότερες ετικέτες που χαρακτηρίζουν το είδος της ταινίας και τέλος τη σύνοψη της υπόθεσής της. Αρχικά εισάγουμε το dataset στο dataframe `df_data_1`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62SOj46gPSyS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_url = 'data/movieDBe.csv'\n",
    "df_data_1 = pd.read_csv(dataset_url, sep='\\t',  header=None, quoting=3, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θα δουλέψουμε σε ένα υποσύνολο 5.000 ταινιών ανάλογα με το seed της ομάδας μας:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2POlqDjkPSyY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Seed Number: 11\n",
    "team_seed_number = 11\n",
    "\n",
    "movie_seeds_url = 'data/seeds2019.csv'\n",
    "df_data_2 = pd.read_csv(movie_seeds_url, header=None, error_bad_lines=False)\n",
    "\n",
    "my_index = df_data_2.iloc[team_seed_number, :].values\n",
    "\n",
    "titles = df_data_1.iloc[:, [2]].values[my_index]\n",
    "categories = df_data_1.iloc[:, [3]].values[my_index]\n",
    "bins = df_data_1.iloc[:, [4]]\n",
    "catbins = bins[4].str.split(',', expand=True).values.astype(np.float)[my_index]\n",
    "summaries =  df_data_1.iloc[:, [5]].values[my_index] \n",
    "corpus = summaries[:, 0].tolist() # list form of summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "If66lkwxPSyb"
   },
   "source": [
    "- Ο πίνακας **titles** περιέχει τους τίτλους των ταινιών.\n",
    "\n",
    "- O πίνακας **categories** περιέχει τις κατηγορίες της ταινίας υπό τη μορφή string. Παρατηρούμε ότι είναι μια comma-separated λίστα strings, με κάθε string να είναι μια κατηγορία.\n",
    "\n",
    "- Ο πίνακας **catbins** περιλαμβάνει πάλι τις κατηγορίες των ταινιών αλλά σε δυαδική μορφή μέσω *One-Hot-Encoding*.\n",
    "\n",
    "- Ο πίνακας **summaries** και η λίστα **corpus** περιλαμβάνουν τις συνόψεις των ταινιών με την corpus είναι ο πίνακας summaries σε μορφή λίστας. Κάθε σύνοψη είναι σε μορφή string.\n",
    "\n",
    "- Θεωρούμε ως **id** της κάθε ταινίας τον αριθμό γραμμής της ή το αντίστοιχο στοιχείο της λίστας."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rnA2RP8GPSyf"
   },
   "source": [
    "Στη συνέχεια της άσκησης θα υλοποιήσουμε ένα *Content Based Recommender System*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DD5KuSKrxQ8I"
   },
   "source": [
    "### Μετατροπή σε TF - IDF\n",
    "\n",
    "Το πρώτο βήμα θα είναι λοιπόν να μετατρέψουμε το corpus σε αναπαράσταση *Term Frequency-Inverse Document Frequency (TD-IDF)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5YP6XCZPSyh"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "corpus_tf_idf = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-uRZK3EPSyl"
   },
   "source": [
    "Σημειώνουμε πως η συνάρτηση [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) όπως καλείται εδώ δεν είναι βελτιστοποιημένη."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 801,
     "status": "ok",
     "timestamp": 1575901068438,
     "user": {
      "displayName": "Γιώργος Αλεξανδρίδης",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAHgjbkpnVZpMkbe7ihkb3km8rd5_Hm0fOfRa_yBg=s64",
      "userId": "16422904839632337525"
     },
     "user_tz": -120
    },
    "id": "y_Cw0brpnisF",
    "outputId": "f8c6d54d-7ee0-474f-e55c-0fe497454b8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 48526)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tf_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Υλοποίηση Συστήματος Συστάσεων\n",
    "\n",
    "Θα δημιουργήσουμε τη συνάρτηση `content_recommender` με ορίσματα `target_movie` και `max_recommendations`. Πιο συγκεκριμένα, θα υπολογίζουμε για κάθε ταινία-στόχο την [ομοιότητα συνημιτόνου](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) από το `corpus_tf_idf` με όλες τις υπόλοιπες ταινίες, ενώ στη συνέχεια θα ταξινομούμε τις αντίστοιχες ομοιότητες ως προς το *index* της εκάστοτε ταινίας. Η συνάρτησή μας θα επιστρέφει το id, τον τίτλο, την κατηγορία και τη σύνοψη τόσο της ταινίας-στόχου, όσο και των `max_recommendations` τω πλήθος συναφών ταινιών."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def content_recommender(target_movie, max_recommendations):\n",
    "    \"\"\" Returns ID, Title, Genre and Summary of target_movie and #max_recommendation movies respectively. \n",
    "    Metric used: Cosine Similarity. \"\"\"\n",
    "    \n",
    "    target_index = np.where(titles == target_movie)[0][0] # fetch target movie index\n",
    "    distances = np.zeros((corpus_tf_idf.shape[0]))\n",
    "    for i in range(corpus_tf_idf.shape[0]):\n",
    "        distances[i] = cosine_similarity(corpus_tf_idf[target_index], corpus_tf_idf[i])\n",
    "        \n",
    "#     distances_values = np.sort(distances) # ascending order of values\n",
    "    distances_slices = np.argsort(distances) # ascending order of indexes\n",
    "#     distances_values_descending = distances_values[::-1] # reverse\n",
    "    distances_slices_descending = distances_slices[::-1]\n",
    "    \n",
    "    result = distances_slices_descending[:max_recommendations+1]\n",
    "#     for i in range(max_recommendations+1):\n",
    "#         if distances_values_descending[i] >= 0.25: # keep results with similarity higher than 0.25\n",
    "#             result.append(distances_slices_descending[i])\n",
    "        \n",
    "    for movie in result:\n",
    "        yield(\"Movie Title: \" + titles[movie][0] + \" - Genre: \" + categories[movie][0] + \" ID: \" + str(movie) +\n",
    "              \"\\n\" + summaries[movie][0] + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8IvHkTUHyu78"
   },
   "source": [
    "### Βελτιστοποίηση"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Προς βελτίωση της παραπάνω συνάρτησης συστάσεων θα ακολουθήσουμε αρχικά τη διαδικασία [Stemming](https://en.wikipedia.org/wiki/Stemming). Για αυτό το σκοπό θα χρησιμοποιήσουμε τον `PorterStemmer` από το πακέτο `nltk`. Αποτέλεσμα της διαδικασίας θα είναι η λίστα `corpus_stem`, η οποία βρίσκεται στην ακριβώς ίδια μορφή με την αρχική λίστα `corpus`, με τη μοναδικά διαφορά εκείνη των επεξεργασμένων λέξεων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# https://bit.ly/35E3kqz\n",
    "corpus_stem_split = [[porter_stemmer.stem(word) for word in summary.split(\" \")] for summary in corpus]\n",
    "corpus_stem = [\" \".join(sentence) for sentence in corpus_stem_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Συνεχίζουμε με την υλοποίηση της συνάρτηση `fetch_most_frequend_words`, η οποία επιστρέφει τις `n` τω πλήθος πιο συχνές λέξεις των συνόψεων των ταινιών μας. Σημειώνουμε ότι από τις παραπάνω λέξεις θα αγνοήσουμε εκείνες που ανήκουν στο σύνολο `nltk.stopwords`, ώστε να προσαρμόσουμε τα αποτελέσματά μας στα πλαίσια των ταινιών και κατά συνέπεια στα δεδομένα μας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def fetch_most_frequent_words(corpus, n=None):\n",
    "    \"\"\" Returns a list of most common words found in given corpus plus theirrespective values.\n",
    "    Stopwords for nltk package are excluded. \"\"\"\n",
    "    \n",
    "    vec = CountVectorizer(stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    \n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αρχικά θα εξετάσουμε τις 500 πιο συχνές λέξεις, τις οποίες θα αξιολογήσουμε ώστε να αποφανθούμε σχετικά με την εισαγωγή τους στο τελικό σύνολο stopwords που θα χρησιμοποιήσουμε. Κατά την αξιολόγηση θα προσπαθήσουμε να συμπεριλάβουμε εκείνες μονάχα τις λέξεις που η αφαίρεσή τους δε θα αλλοιώνει το βασικό πλαίσιο της ταινίας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hi', 23051), ('ha', 6488), ('wa', 3858), ('kill', 3386), ('tell', 3036), ('tri', 2764), ('film', 2739), ('thi', 2627), ('father', 2596), ('make', 2532), ('love', 2468), ('man', 2324), ('leav', 2309), ('time', 2255), ('becom', 2250), ('come', 2176), ('return', 2133), ('new', 2122), ('onli', 2028), ('meet', 1981), ('life', 1981), ('help', 1966), ('day', 1961), ('home', 1955), ('end', 1943), ('friend', 1933), ('befor', 1929), ('later', 1928), ('begin', 1816), ('decid', 1790), ('mother', 1781), ('work', 1755), ('use', 1743), ('live', 1655), ('year', 1631), ('run', 1613), ('young', 1606), ('goe', 1576), ('ask', 1485), ('wife', 1484), ('fall', 1480), ('attempt', 1479), ('son', 1477), ('reveal', 1445), ('away', 1444), ('dure', 1443), ('turn', 1442), ('girl', 1423), ('start', 1415), ('way', 1405), ('discov', 1395), ('old', 1393), ('night', 1381), ('want', 1374), ('final', 1348), ('polic', 1345), ('famili', 1340), ('arriv', 1323), ('death', 1318), ('car', 1285), ('fight', 1274), ('attack', 1261), ('daughter', 1257), ('say', 1255), ('woman', 1247), ('forc', 1226), ('follow', 1218), ('head', 1188), ('brother', 1186), ('money', 1184), ('plan', 1181), ('play', 1172), ('becaus', 1168), ('soon', 1160), ('set', 1152), ('men', 1148), ('know', 1146), ('marri', 1136), ('order', 1133), ('hous', 1127), ('just', 1127), ('stori', 1109), ('murder', 1105), ('town', 1105), ('look', 1102), ('escap', 1096), ('learn', 1067), ('doe', 1066), ('anoth', 1047), ('boy', 1044), ('realiz', 1034), ('school', 1031), ('place', 1002), ('like', 981), ('group', 972), ('lead', 965), ('world', 927), ('believ', 922), ('die', 917), ('manag', 903), ('room', 901), ('save', 890), ('eventu', 885), ('stop', 882), ('caus', 862), ('shoot', 850), ('visit', 848), ('open', 846), ('dr', 843), ('dead', 842), ('children', 836), ('husband', 829), ('hand', 809), ('sister', 808), ('jack', 807), ('house', 805), ('break', 789), ('scene', 781), ('war', 780), ('local', 771), ('sever', 770), ('tom', 767), ('onc', 766), ('feel', 758), ('bring', 756), ('left', 749), ('appear', 747), ('peopl', 738), ('continu', 727), ('job', 726), ('train', 714), ('offer', 707), ('stay', 695), ('face', 692), ('offic', 680), ('refus', 670), ('includ', 665), ('explain', 663), ('team', 662), ('long', 656), ('family', 654), ('inform', 654), ('convinc', 653), ('john', 647), ('send', 646), ('gang', 646), ('drive', 642), ('think', 642), ('gun', 636), ('relationship', 636), ('veri', 629), ('walk', 626), ('child', 625), ('agre', 618), ('join', 618), ('state', 617), ('talk', 617), ('power', 609), ('chang', 605), ('ship', 602), ('taken', 591), ('chase', 588), ('instead', 586), ('mr', 582), ('confront', 579), ('travel', 571), ('movi', 570), ('littl', 570), ('shot', 568), ('person', 565), ('mani', 563), ('small', 563), ('good', 559), ('destroy', 559), ('win', 558), ('search', 553), ('american', 551), ('need', 551), ('villag', 550), ('enter', 548), ('bodi', 547), ('claim', 545), ('sinc', 542), ('ani', 541), ('dog', 541), ('land', 539), ('let', 536), ('close', 536), ('watch', 536), ('allow', 535), ('despit', 532), ('thing', 532), ('success', 526), ('lose', 522), ('rescu', 518), ('accident', 515), ('point', 514), ('doesn', 514), ('doctor', 512), ('parent', 511), ('prison', 510), ('real', 508), ('high', 507), ('secret', 505), ('pass', 504), ('student', 504), ('water', 503), ('hear', 502), ('david', 502), ('togeth', 501), ('captur', 499), ('receiv', 499), ('steal', 496), ('remain', 495), ('door', 495), ('king', 495), ('mari', 494), ('hope', 488), ('abl', 485), ('result', 483), ('hold', 480), ('accept', 477), ('hit', 477), ('hide', 474), ('perform', 472), ('act', 471), ('human', 471), ('citi', 462), ('involv', 461), ('miss', 461), ('member', 459), ('reach', 458), ('game', 458), ('arrest', 453), ('black', 452), ('catch', 452), ('actual', 451), ('charli', 451), ('parti', 450)]\n"
     ]
    }
   ],
   "source": [
    "most_freq = []\n",
    "for word, freq in fetch_most_frequent_words(corpus_stem, 500):\n",
    "    if freq >= 450:\n",
    "        most_freq.append((word, freq))\n",
    "print(most_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Μελετώντας τα παραπάνω θα δημιουργήσουμε τη λίστα `usefull`, στην οποία θα προσθέσουμε λέξεις που θεωρούμε πως έχουν βαρύτητα στην υπόθεση της ταινίας και δε θα ήταν αποδοτικό να αφαιρεθούν κατά την υλοποίηση του συστήματος συστάσεών μας. Στη συνέχεια, θα εμπλουτίσουμε τη λίστα `acc` με όσες λέξεις βρήκαμε παραπάνω και τελικά θα αφαιρέσουμε τις χρήσιμες. Σημειώνουμε ότι η λίστα `acc` αποτελεί ένα έτοιμο σύνολο από stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CONTINUE HERE! '''\n",
    "\n",
    "# Stopwords source: https://bit.ly/2QYET1x\n",
    "acc = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"A\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"aj\", \"al\", \"all\", \"articl\", \"research\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appreciate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"arise\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"B\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"been\", \"before\", \"beforehand\", \"beginnings\", \"behind\", \"below\", \"beside\", \"besides\", \"best\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"C\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"could\", \"couldn\", \"couldnt\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"D\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doing\", \"don\", \"done\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"E\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"F\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"G\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"H\", \"h2\", \"h3\", \"had\", \"hadn\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"have\", \"haven\", \"having\", \"he\", \"hed\", \"hello\", \"help\", \"hence\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hes\", \"hh\", \"hi\", \"hid\", \"hither\", \"hj\", \"ho\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"im\", \"immediately\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"it\", \"itd\", \"its\", \"iv\", \"ix\", \"iy\", \"iz\", \"j\", \"J\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"K\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"ko\", \"l\", \"L\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"M\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"my\", \"n\", \"N\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"neither\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"O\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"otherwise\", \"ou\", \"ought\", \"our\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"P\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"pp\", \"pq\", \"pr\", \"character\", \"plot\", \"sets\", \"set\", \"events\", \"narrating\", \"scene\", \"people\", \"time\", \"predominantly\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"Q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"R\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"S\", \"s2\", \"sa\", \"said\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"sent\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somehow\", \"somethan\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"sz\", \"t\", \"T\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"these\", \"they\", \"theyd\", \"theyre\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"U\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"using\", \"usually\", \"ut\", \"v\", \"V\", \"va\", \"various\", \"vd\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"W\", \"wa\", \"was\", \"wasn\", \"wasnt\", \"way\", \"we\", \"wed\", \"welcome\", \"well\", \"well-b\", \"went\", \"were\", \"weren\", \"werent\", \"what\", \"whatever\", \"whats\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"whom\", \"whomever\", \"whos\", \"whose\", \"why\", \"wi\", \"widely\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"would\", \"wouldn\", \"wouldnt\", \"www\", \"x\", \"X\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"Y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"your\", \"youre\", \"yours\", \"yr\", \"ys\", \"yt\", \"z\", \"Z\", \"zero\", \"zi\", \"zz\"]\n",
    "\n",
    "usefull = ['father', 'love', 'life', 'home', 'family', 'house', 'mother', 'police', 'wife', 'son', 'killed', 'death', 'car',\n",
    "       'daughter', 'friend', 'friends', 'money', 'kill', 'school', 'brother', 'group', 'world', 'fight', 'escape', 'return',\n",
    "       'work', 'dead', 'children', 'city', 'husband', 'boy', 'war', 'attempts', 'sister', 'parents', 'local', 'job', 'party',\n",
    "       'body', 'kills', 'plan', 'run', 'murder', 'refuses', 'village', 'child', 'hospital', 'gang', 'team', 'married',\n",
    "       'relationship', 'marry', 'attack', 'realizes', 'know', 'marriage', 'killing', 'attempt', 'gun', 'baby', 'train', 'hand',\n",
    "       'working', 'apartment', 'manages', 'shot', 'ship', 'face', 'agrees', 'american', 'army', 'king', 'officer', 'water', \n",
    "       'couple', 'company', 'use', 'forced', 'dog', 'save', 'door', 'secret', 'dies']\n",
    "\n",
    "most_freq_words = []\n",
    "for word in most_freq:\n",
    "    most_freq_words.append(word[0])\n",
    "\n",
    "final_acc = set(acc + most_freq_words)\n",
    "for word in usefull:\n",
    "    final_acc.remove(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σε αυτό το σημείο ολοκληρώσαμε τη διαδικασία της προεπεξεργασίας των δεδομένων μας, δηλαδή της λίστας `corpus`. Ακολουθεί η βελτιστοποίση του `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words=final_acc, max_df=0.90, strip_accents='unicode')\n",
    "vectorizer.fit(corpus)\n",
    "corpus_tf_idf = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tf_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NPVK7Z5c1p5F"
   },
   "source": [
    "### Επεξήγηση Επιλογών και Ποιοτική Ερμηνεία\n",
    "\n",
    "Σε markdown περιγράψτε πώς προχωρήσατε στις επιλογές σας για τη βελτιστοποίηση της `TfidfVectorizer`. Επίσης σε markdown δώστε 10 παραδείγματα (IDs) από τη συλλογή σας που επιστρέφουν καλά αποτελέσματα μέχρι `max_recommendations` (5 και παραπάνω) και σημειώστε συνοπτικά ποια είναι η θεματική που ενώνει τις ταινίες.\n",
    "\n",
    "Δείτε [εδώ](https://pastebin.com/raw/ZEvg5t3z) ένα παράδειγμα εξόδου του βελτιστοποιημένου συστήματος συστάσεων για την ταίνία [\"Q Planes\"](https://en.wikipedia.org/wiki/Q_Planes) με την κλήση της συνάρτησης για κάποιο seed `content_recommender(529,3)`. Είναι φανερό ότι η κοινή θεματική των ταινιών είναι τα αεροπλάνα, οι πτήσεις, οι πιλότοι, ο πόλεμος."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4irg4K-IPSym"
   },
   "source": [
    "## Tip: persistence αντικειμένων με joblib.dump\n",
    "\n",
    "H βιβλιοθήκη [joblib](https://pypi.python.org/pypi/joblib) της Python δίνει κάποιες εξαιρετικά χρήσιμες ιδιότητες στην ανάπτυξη κώδικα: pipelining, παραλληλισμό, caching και variable persistence. Τις τρεις πρώτες ιδιότητες τις είδαμε στην πρώτη άσκηση. Στην παρούσα άσκηση θα μας φανεί χρήσιμη η τέταρτη, το persistence των αντικειμένων. Συγκεκριμένα μπορούμε με:\n",
    "\n",
    "```python\n",
    "joblib.dump(my_object, 'my_object.pkl') \n",
    "```\n",
    "\n",
    "να αποθηκεύσουμε οποιοδήποτε αντικείμενο-μεταβλητή (εδώ το `my_object`) απευθείας πάνω στο filesystem ως αρχείο, το οποίο στη συνέχεια μπορούμε να ανακαλέσουμε ως εξής:\n",
    "\n",
    "```python\n",
    "my_object = joblib.load('my_object.pkl')\n",
    "```\n",
    "\n",
    "Μπορούμε έτσι να ανακαλέσουμε μεταβλητές ακόμα και αφού κλείσουμε και ξανανοίξουμε το notebook, χωρίς να χρειαστεί να ακολουθήσουμε ξανά όλα τα βήματα ένα - ένα για την παραγωγή τους, κάτι ιδιαίτερα χρήσιμο αν αυτή η διαδικασία είναι χρονοβόρα. Προσοχή: αυτό ισχύει μόνο στα Azure και Kaggle, στο Colab και στο IBM τα αρχεία εξαφανίζονται όταν ανακυκλώνεται ο πυρήνας και θα πρέπει να τα αποθηκεύετε τοπικά. Περισσότερα στο [FAQ](https://docs.google.com/document/d/1-E4eQkVnTxa3Jb0HL9OAs11bugYRRZ7RNWpu7yh9G4s/edit?usp=sharing).\n",
    "\n",
    "Ας αποθηκεύσουμε το `corpus_tf_idf` και στη συνέχεια ας το ανακαλέσουμε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1304,
     "status": "ok",
     "timestamp": 1575901456994,
     "user": {
      "displayName": "Γιώργος Αλεξανδρίδης",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAHgjbkpnVZpMkbe7ihkb3km8rd5_Hm0fOfRa_yBg=s64",
      "userId": "16422904839632337525"
     },
     "user_tz": -120
    },
    "id": "aESOPYQaPSyo",
    "outputId": "b5adeaee-dd1e-4de3-b41b-9bbdd2f1a5ff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(corpus_tf_idf, 'corpus_tf_idf.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_rAEj5ZPSy1"
   },
   "source": [
    "\n",
    "\n",
    "Μπορείτε με ένα απλό `!ls` να δείτε ότι το αρχείο `corpus_tf_idf.pkl` υπάρχει στο filesystem σας (== persistence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5073,
     "status": "ok",
     "timestamp": 1575901466223,
     "user": {
      "displayName": "Γιώργος Αλεξανδρίδης",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAHgjbkpnVZpMkbe7ihkb3km8rd5_Hm0fOfRa_yBg=s64",
      "userId": "16422904839632337525"
     },
     "user_tz": -120
    },
    "id": "ZhwXmTEIPSy3",
    "outputId": "f3e07f49-db04-46bf-9661-e0e5f4902839",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cey5AbkO475S"
   },
   "source": [
    "και μπορούμε να τα διαβάσουμε με `joblib.load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSJPTKY8PSyu"
   },
   "outputs": [],
   "source": [
    "corpus_tf_idf = joblib.load('corpus_tf_idf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zHOQtO83PSy9"
   },
   "source": [
    "## 2. Τοπολογική και σημασιολογική απεικόνιση ταινιών με χρήση SOM\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=1R1R7Ds9UEfhjOY_fk_3wcTjsM0rI4WLl\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UB_clmizPSy-"
   },
   "source": [
    "## Δημιουργία dataset\n",
    "Στη δεύτερη εφαρμογή θα βασιστούμε στις τοπολογικές ιδιότητες των Self Organizing Maps (SOM) για να φτιάξουμε ενά χάρτη (grid) δύο διαστάσεων όπου θα απεικονίζονται όλες οι ταινίες της συλλογής της ομάδας με τρόπο χωρικά συνεκτικό ως προς το περιεχόμενο και κυρίως το είδος τους. \n",
    "\n",
    "Η `build_final_set` αρχικά μετατρέπει την αραιή αναπαράσταση tf-idf της εξόδου της `TfidfVectorizer()` σε πυκνή (η [αραιή αναπαράσταση](https://en.wikipedia.org/wiki/Sparse_matrix) έχει τιμές μόνο για τα μη μηδενικά στοιχεία). \n",
    "\n",
    "Στη συνέχεια ενώνει την πυκνή `dense_tf_idf` αναπαράσταση και τις binarized κατηγορίες `catbins` των ταινιών ως επιπλέον στήλες (χαρακτηριστικά). Συνεπώς, κάθε ταινία αναπαρίσταται στο Vector Space Model από τα χαρακτηριστικά του TFIDF και τις κατηγορίες της.\n",
    "\n",
    "Τέλος, δέχεται ένα ορισμα για το πόσες ταινίες να επιστρέψει, με default τιμή όλες τις ταινίες (5000). Αυτό είναι χρήσιμο για να μπορείτε αν θέλετε να φτιάχνετε μικρότερα σύνολα δεδομένων ώστε να εκπαιδεύεται ταχύτερα το SOM.\n",
    "\n",
    "Σημειώστε ότι το IBM Watson δείνει \"Kernel dead\" εάν δεν έχετε βελτιστοποιήσει το tfidf και μικρύνει τις διαστάσεις του dataset (πιθανότατα κάποια υπέρβαση μνήμης)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-FDDOkQPSzA"
   },
   "outputs": [],
   "source": [
    "def build_final_set(doc_limit = 5000, tf_idf_only=False):\n",
    "    # convert sparse tf_idf to dense tf_idf representation\n",
    "    dense_tf_idf = corpus_tf_idf.toarray()[0:doc_limit,:]\n",
    "    if tf_idf_only:\n",
    "        # use only tf_idf\n",
    "        final_set = dense_tf_idf\n",
    "    else:\n",
    "        # append the binary categories features horizontaly to the (dense) tf_idf features\n",
    "        final_set = np.hstack((dense_tf_idf, catbins[0:doc_limit,:]))\n",
    "        # η somoclu θέλει δεδομένα σε float32\n",
    "    return np.array(final_set, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aF1B62UbPSzF"
   },
   "outputs": [],
   "source": [
    "final_set = build_final_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KjvPPENS_dYL"
   },
   "source": [
    "Τυπώνουμε τις διαστάσεις του τελικού dataset μας. Χωρίς βελτιστοποίηση του TFIDF θα έχουμε περίπου 50.000 χαρακτηριστικά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 766,
     "status": "ok",
     "timestamp": 1575901615232,
     "user": {
      "displayName": "Γιώργος Αλεξανδρίδης",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAHgjbkpnVZpMkbe7ihkb3km8rd5_Hm0fOfRa_yBg=s64",
      "userId": "16422904839632337525"
     },
     "user_tz": -120
    },
    "id": "fvEgNn-L-jEw",
    "outputId": "de1a7e16-3689-4d38-bc93-f264b57a85b4"
   },
   "outputs": [],
   "source": [
    "final_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Om7PoyDVCqis"
   },
   "source": [
    "Με βάση την εμπειρία σας στην προετοιμασία των δεδομένων στην επιβλεπόμενη μάθηση, υπάρχει κάποιο βήμα προεπεξεργασίας που θα μπορούσε να εφαρμοστεί σε αυτό το dataset; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tikdip0PSzQ"
   },
   "source": [
    "## Εκπαίδευση χάρτη SOM\n",
    "\n",
    "Θα δουλέψουμε με τη βιβλιοθήκη SOM [\"Somoclu\"](http://somoclu.readthedocs.io/en/stable/index.html). Εισάγουμε τις somoclu και matplotlib και λέμε στη matplotlib να τυπώνει εντός του notebook (κι όχι σε pop up window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oX9rzxGSPSzR"
   },
   "outputs": [],
   "source": [
    "# install somoclu\n",
    "!pip install --upgrade somoclu\n",
    "# import sompoclu, matplotlib\n",
    "import somoclu\n",
    "import matplotlib\n",
    "# we will plot inside the notebook and not in separate window\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EqBfn0ijPSzX"
   },
   "source": [
    "Καταρχάς διαβάστε το [function reference](http://somoclu.readthedocs.io/en/stable/reference.html) του somoclu. Θα δoυλέψουμε με χάρτη τύπου planar, παραλληλόγραμμου σχήματος νευρώνων με τυχαία αρχικοποίηση (όλα αυτά είναι default). Μπορείτε να δοκιμάσετε διάφορα μεγέθη χάρτη ωστόσο όσο ο αριθμός των νευρώνων μεγαλώνει, μεγαλώνει και ο χρόνος εκπαίδευσης. Για το training δεν χρειάζεται να ξεπεράσετε τα 100 epochs. Σε γενικές γραμμές μπορούμε να βασιστούμε στις default παραμέτρους μέχρι να έχουμε τη δυνατότητα να οπτικοποιήσουμε και να αναλύσουμε ποιοτικά τα αποτελέσματα. Ξεκινήστε με ένα χάρτη 10 x 10, 100 epochs training και ένα υποσύνολο των ταινιών (π.χ. 2000). Χρησιμοποιήστε την `time` για να έχετε μια εικόνα των χρόνων εκπαίδευσης. Ενδεικτικά, με σωστή κωδικοποίηση tf-idf, μικροί χάρτες για λίγα δεδομένα (1000-2000) παίρνουν γύρω στο ένα λεπτό ενώ μεγαλύτεροι χάρτες με όλα τα δεδομένα μπορούν να πάρουν 10-15 λεπτά ή και περισσότερο.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ntd2GE9SaHiS"
   },
   "source": [
    "\n",
    "## Best matching units\n",
    "\n",
    "Μετά από κάθε εκπαίδευση αποθηκεύστε σε μια μεταβλητή τα best matching units (bmus) για κάθε ταινία. Τα bmus μας δείχνουν σε ποιο νευρώνα ανήκει η κάθε ταινία. Προσοχή: η σύμβαση των συντεταγμένων των νευρώνων είναι (στήλη, γραμμή) δηλαδή το ανάποδο από την Python. Με χρήση της [np.unique](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.unique.html) (μια πολύ χρήσιμη συνάρτηση στην άσκηση) αποθηκεύστε τα μοναδικά best matching units και τους δείκτες τους (indices) προς τις ταινίες. Σημειώστε ότι μπορεί να έχετε λιγότερα μοναδικά bmus από αριθμό νευρώνων γιατί μπορεί σε κάποιους νευρώνες να μην έχουν ανατεθεί ταινίες. Ως αριθμό νευρώνα θα θεωρήσουμε τον αριθμό γραμμής στον πίνακα μοναδικών bmus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "grzqcyHyaKdg"
   },
   "source": [
    "\n",
    "## Ομαδοποίηση (clustering)\n",
    "\n",
    "Τυπικά, η ομαδοποίηση σε ένα χάρτη SOM προκύπτει από το unified distance matrix (U-matrix): για κάθε κόμβο υπολογίζεται η μέση απόστασή του από τους γειτονικούς κόμβους. Εάν χρησιμοποιηθεί μπλε χρώμα στις περιοχές του χάρτη όπου η τιμή αυτή είναι χαμηλή (μικρή απόσταση) και κόκκινο εκεί που η τιμή είναι υψηλή (μεγάλη απόσταση), τότε μπορούμε να πούμε ότι οι μπλε περιοχές αποτελούν clusters και οι κόκκινες αποτελούν σύνορα μεταξύ clusters.\n",
    "\n",
    "To somoclu δίνει την επιπρόσθετη δυνατότητα να κάνουμε ομαδοποίηση των νευρώνων χρησιμοποιώντας οποιονδήποτε αλγόριθμο ομαδοποίησης του scikit-learn. Στην άσκηση θα χρησιμοποιήσουμε τον k-Means. Για τον αρχικό σας χάρτη δοκιμάστε ένα k=20 ή 25. Οι δύο προσεγγίσεις ομαδοποίησης είναι διαφορετικές, οπότε περιμένουμε τα αποτελέσματα να είναι κοντά αλλά όχι τα ίδια.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nupuqcuaMe3"
   },
   "source": [
    "\n",
    "## Αποθήκευση του SOM\n",
    "\n",
    "Επειδή η αρχικοποίηση του SOM γίνεται τυχαία και το clustering είναι και αυτό στοχαστική διαδικασία, οι θέσεις και οι ετικέτες των νευρώνων και των clusters θα είναι διαφορετικές κάθε φορά που τρέχετε τον χάρτη, ακόμα και με τις ίδιες παραμέτρους. Για να αποθηκεύσετε ένα συγκεκριμένο som και clustering χρησιμοποιήστε και πάλι την `joblib`. Μετά την ανάκληση ενός SOM θυμηθείτε να ακολουθήσετε τη διαδικασία για τα bmus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ejX0Qs18aRHU"
   },
   "source": [
    "\n",
    "## Οπτικοποίηση U-matrix, clustering και μέγεθος clusters\n",
    "\n",
    "Για την εκτύπωση του U-matrix χρησιμοποιήστε τη `view_umatrix` με ορίσματα `bestmatches=True` και `figsize=(15, 15)` ή `figsize=(20, 20)`. Τα διαφορετικά χρώματα που εμφανίζονται στους κόμβους αντιπροσωπεύουν τα διαφορετικά clusters που προκύπτουν από τον k-Means. Μπορείτε να εμφανίσετε τη λεζάντα του U-matrix με το όρισμα `colorbar`. Μην τυπώνετε τις ετικέτες (labels) των δειγμάτων, είναι πολύ μεγάλος ο αριθμός τους.\n",
    "\n",
    "Για μια δεύτερη πιο ξεκάθαρη οπτικοποίηση του clustering τυπώστε απευθείας τη μεταβλητή `clusters`.\n",
    "\n",
    "Τέλος, χρησιμοποιώντας πάλι την `np.unique` (με διαφορετικό όρισμα) και την `np.argsort` (υπάρχουν και άλλοι τρόποι υλοποίησης) εκτυπώστε τις ετικέτες των clusters (αριθμοί από 0 έως k-1) και τον αριθμό των νευρώνων σε κάθε cluster, με φθίνουσα ή αύξουσα σειρά ως προς τον αριθμό των νευρώνων. Ουσιαστικά είναι ένα εργαλείο για να βρίσκετε εύκολα τα μεγάλα και μικρά clusters. \n",
    "\n",
    "Ακολουθεί ένα μη βελτιστοποιημένο παράδειγμα για τις τρεις προηγούμενες εξόδους:\n",
    "\n",
    "<img src=\"https://image.ibb.co/i0tsfR/umatrix_s.jpg\" width=\"35%\">\n",
    "<img src=\"https://image.ibb.co/nLgHEm/clusters.png\" width=\"35%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fMO_KcQYaTv-"
   },
   "source": [
    "\n",
    "## Σημασιολογική ερμηνεία των clusters\n",
    "\n",
    "Προκειμένου να μελετήσουμε τις τοπολογικές ιδιότητες του SOM και το αν έχουν ενσωματώσει σημασιολογική πληροφορία για τις ταινίες διαμέσου της διανυσματικής αναπαράστασης με το tf-idf και των κατηγοριών, χρειαζόμαστε ένα κριτήριο ποιοτικής επισκόπησης των clusters. Θα υλοποιήσουμε το εξής κριτήριο: Λαμβάνουμε όρισμα έναν αριθμό (ετικέτα) cluster. Για το cluster αυτό βρίσκουμε όλους τους νευρώνες που του έχουν ανατεθεί από τον k-Means. Για όλους τους νευρώνες αυτούς βρίσκουμε όλες τις ταινίες που τους έχουν ανατεθεί (για τις οποίες αποτελούν bmus). Για όλες αυτές τις ταινίες τυπώνουμε ταξινομημένη τη συνολική στατιστική όλων των ειδών (κατηγοριών) και τις συχνότητές τους. Αν το cluster διαθέτει καλή συνοχή και εξειδίκευση, θα πρέπει κάποιες κατηγορίες να έχουν σαφώς μεγαλύτερη συχνότητα από τις υπόλοιπες. Θα μπορούμε τότε να αναθέσουμε αυτήν/ές την/τις κατηγορία/ες ως ετικέτες κινηματογραφικού είδους στο cluster.\n",
    "\n",
    "Μπορείτε να υλοποιήσετε τη συνάρτηση αυτή όπως θέλετε. Μια πιθανή διαδικασία θα μπορούσε να είναι η ακόλουθη:\n",
    "\n",
    "1. Ορίζουμε συνάρτηση `print_categories_stats` που δέχεται ως είσοδο λίστα με ids ταινιών. Δημιουργούμε μια κενή λίστα συνολικών κατηγοριών. Στη συνέχεια, για κάθε ταινία επεξεργαζόμαστε το string `categories` ως εξής: δημιουργούμε μια λίστα διαχωρίζοντας το string κατάλληλα με την `split` και αφαιρούμε τα whitespaces μεταξύ ετικετών με την `strip`. Προσθέτουμε τη λίστα αυτή στη συνολική λίστα κατηγοριών με την `extend`. Τέλος χρησιμοποιούμε πάλι την `np.unique` για να μετρήσουμε συχνότητα μοναδικών ετικετών κατηγοριών και ταξινομούμε με την `np.argsort`. Τυπώνουμε τις κατηγορίες και τις συχνότητες εμφάνισης ταξινομημένα. Χρήσιμες μπορεί να σας φανούν και οι `np.ravel`, `np.nditer`, `np.array2string` και `zip`.\n",
    "\n",
    "2. Ορίζουμε τη βασική μας συνάρτηση `print_cluster_neurons_movies_report` που δέχεται ως όρισμα τον αριθμό ενός cluster. Με τη χρήση της `np.where` μπορούμε να βρούμε τις συντεταγμένες των bmus που αντιστοιχούν στο cluster και με την `column_stack` να φτιάξουμε έναν πίνακα bmus για το cluster. Προσοχή στη σειρά (στήλη - σειρά) στον πίνακα bmus. Για κάθε bmu αυτού του πίνακα ελέγχουμε αν υπάρχει στον πίνακα μοναδικών bmus που έχουμε υπολογίσει στην αρχή συνολικά και αν ναι προσθέτουμε το αντίστοιχο index του νευρώνα σε μια λίστα. Χρήσιμες μπορεί να είναι και οι `np.rollaxis`, `np.append`, `np.asscalar`. Επίσης πιθανώς να πρέπει να υλοποιήσετε ένα κριτήριο ομοιότητας μεταξύ ενός bmu και ενός μοναδικού bmu από τον αρχικό πίνακα bmus.\n",
    "\n",
    "3. Υλοποιούμε μια βοηθητική συνάρτηση `neuron_movies_report`. Λαμβάνει ένα σύνολο νευρώνων από την `print_cluster_neurons_movies_report` και μέσω της `indices` φτιάχνει μια λίστα με το σύνολο ταινιών που ανήκουν σε αυτούς τους νευρώνες. Στο τέλος καλεί με αυτή τη λίστα την `print_categories_stats` που τυπώνει τις στατιστικές των κατηγοριών.\n",
    "\n",
    "Μπορείτε βέβαια να προσθέσετε οποιαδήποτε επιπλέον έξοδο σας βοηθάει. Μια χρήσιμη έξοδος είναι πόσοι νευρώνες ανήκουν στο cluster και σε πόσους και ποιους από αυτούς έχουν ανατεθεί ταινίες.\n",
    "\n",
    "Θα επιτελούμε τη σημασιολογική ερμηνεία του χάρτη καλώντας την `print_cluster_neurons_movies_report` με τον αριθμός ενός cluster που μας ενδιαφέρει. \n",
    "\n",
    "Παράδειγμα εξόδου για ένα cluster (μη βελτιστοποιημένος χάρτης, ωστόσο βλέπετε ότι οι μεγάλες κατηγορίες έχουν σημασιολογική  συνάφεια):\n",
    "\n",
    "```\n",
    "Overall Cluster Genres stats:  \n",
    "[('\"Horror\"', 86), ('\"Science Fiction\"', 24), ('\"B-movie\"', 16), ('\"Monster movie\"', 10), ('\"Creature Film\"', 10), ('\"Indie\"', 9), ('\"Zombie Film\"', 9), ('\"Slasher\"', 8), ('\"World cinema\"', 8), ('\"Sci-Fi Horror\"', 7), ('\"Natural horror films\"', 6), ('\"Supernatural\"', 6), ('\"Thriller\"', 6), ('\"Cult\"', 5), ('\"Black-and-white\"', 5), ('\"Japanese Movies\"', 4), ('\"Short Film\"', 3), ('\"Drama\"', 3), ('\"Psychological thriller\"', 3), ('\"Crime Fiction\"', 3), ('\"Monster\"', 3), ('\"Comedy\"', 2), ('\"Western\"', 2), ('\"Horror Comedy\"', 2), ('\"Archaeology\"', 2), ('\"Alien Film\"', 2), ('\"Teen\"', 2), ('\"Mystery\"', 2), ('\"Adventure\"', 2), ('\"Comedy film\"', 2), ('\"Combat Films\"', 1), ('\"Chinese Movies\"', 1), ('\"Action/Adventure\"', 1), ('\"Gothic Film\"', 1), ('\"Costume drama\"', 1), ('\"Disaster\"', 1), ('\"Docudrama\"', 1), ('\"Film adaptation\"', 1), ('\"Film noir\"', 1), ('\"Parody\"', 1), ('\"Period piece\"', 1), ('\"Action\"', 1)]```\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lq4QrImhaa7E"
   },
   "source": [
    "\n",
    "## Tips για το SOM και το clustering\n",
    "\n",
    "- Για την ομαδοποίηση ένα U-matrix καλό είναι να εμφανίζει και μπλε-πράσινες περιοχές (clusters) και κόκκινες περιοχές (ορίων). Παρατηρήστε ποια σχέση υπάρχει μεταξύ αριθμού ταινιών στο final set, μεγέθους grid και ποιότητας U-matrix.\n",
    "- Για το k του k-Means προσπαθήστε να προσεγγίζει σχετικά τα clusters του U-matrix (όπως είπαμε είναι διαφορετικοί μέθοδοι clustering). Μικρός αριθμός k δεν θα σέβεται τα όρια. Μεγάλος αριθμός θα δημιουργεί υπο-clusters εντός των clusters που φαίνονται στο U-matrix. Το τελευταίο δεν είναι απαραίτητα κακό, αλλά μεγαλώνει τον αριθμό clusters που πρέπει να αναλυθούν σημασιολογικά.\n",
    "- Σε μικρούς χάρτες και με μικρά final sets δοκιμάστε διαφορετικές παραμέτρους για την εκπαίδευση του SOM. Σημειώστε τυχόν παραμέτρους που επηρεάζουν την ποιότητα του clustering για το dataset σας ώστε να τις εφαρμόσετε στους μεγάλους χάρτες.\n",
    "- Κάποια τοπολογικά χαρακτηριστικά εμφανίζονται ήδη σε μικρούς χάρτες. Κάποια άλλα χρειάζονται μεγαλύτερους χάρτες. Δοκιμάστε μεγέθη 20x20, 25x25 ή και 30x30 και αντίστοιχη προσαρμογή των k. Όσο μεγαλώνουν οι χάρτες, μεγαλώνει η ανάλυση του χάρτη αλλά μεγαλώνει και ο αριθμός clusters που πρέπει να αναλυθούν.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x4IUl8O8ayVf"
   },
   "source": [
    "\n",
    "\n",
    "## Ανάλυση τοπολογικών ιδιοτήτων χάρτη SOM\n",
    "\n",
    "Μετά το πέρας της εκπαίδευσης και του clustering θα έχετε ένα χάρτη με τοπολογικές ιδιότητες ως προς τα είδη των ταίνιών της συλλογής σας, κάτι αντίστοιχο με την εικόνα στην αρχή της Εφαρμογής 2 αυτού του notebook (η συγκεκριμένη εικόνα είναι μόνο για εικονογράφιση, δεν έχει καμία σχέση με τη συλλογή δεδομένων και τις κατηγορίες μας).\n",
    "\n",
    "Για τον τελικό χάρτη SOM που θα παράξετε για τη συλλογή σας, αναλύστε σε markdown με συγκεκριμένη αναφορά σε αριθμούς clusters και τη σημασιολογική ερμηνεία τους τις εξής τρεις τοπολογικές ιδιότητες του SOM: \n",
    "\n",
    "1. Δεδομένα που έχουν μεγαλύτερη πυκνότητα πιθανότητας στο χώρο εισόδου τείνουν να απεικονίζονται με περισσότερους νευρώνες στο χώρο μειωμένης διαστατικότητας. Δώστε παραδείγματα από συχνές και λιγότερο συχνές κατηγορίες ταινιών. Χρησιμοποιήστε τις στατιστικές των κατηγοριών στη συλλογή σας και τον αριθμό κόμβων που χαρακτηρίζουν.\n",
    "2. Μακρινά πρότυπα εισόδου τείνουν να απεικονίζονται απομακρυσμένα στο χάρτη. Υπάρχουν χαρακτηριστικές κατηγορίες ταινιών που ήδη από μικρούς χάρτες τείνουν να τοποθετούνται σε διαφορετικά ή απομονωμένα σημεία του χάρτη.\n",
    "3. Κοντινά πρότυπα εισόδου τείνουν να απεικονίζονται κοντά στο χάρτη. Σε μεγάλους χάρτες εντοπίστε είδη ταινιών και κοντινά τους υποείδη.\n",
    "\n",
    "Προφανώς τοποθέτηση σε 2 διαστάσεις που να σέβεται μια απόλυτη τοπολογία δεν είναι εφικτή, αφενός γιατί δεν υπάρχει κάποια απόλυτη εξ ορισμού για τα κινηματογραφικά είδη ακόμα και σε πολλές διαστάσεις, αφετέρου γιατί πραγματοποιούμε μείωση διαστατικότητας.\n",
    "\n",
    "Εντοπίστε μεγάλα clusters και μικρά clusters που δεν έχουν σαφή χαρακτηριστικά. Εντοπίστε clusters συγκεκριμένων ειδών που μοιάζουν να μην έχουν τοπολογική συνάφεια με γύρω περιοχές. Προτείνετε πιθανές ερμηνείες.\n",
    "\n",
    "\n",
    "\n",
    "Τέλος, εντοπίστε clusters που έχουν κατά την άποψή σας ιδιαίτερο ενδιαφέρον στη συλλογή της ομάδας σας (data exploration / discovery value) και σχολιάστε.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYjxGR5DawIy"
   },
   "source": [
    "\n",
    "# Τελική παράδοση άσκησης\n",
    "\n",
    "- Θα παραδώσετε στο mycourses το παρόν notebook επεξεργασμένο με τις απαντήσεις σας για τα ζητούμενα και των δύο εφαρμογών. \n",
    "- Προσέξτε ώστε να **απαντήσετε σε όλα τα σημεία όπου απαιτείται**.\n",
    "- Μαζί Θα παραδώσετε τα joblib dumps των τελικών `corpus_tf_idf.pkl` και `som.pkl` (μεγάλος χάρτης - 5000 ταινίες) που θα χρησιμοποιήσετε. Θυμηθείτε ότι η ανάλυση του χάρτη στο markdown με αναφορά σε αριθμούς clusters πρέπει να αναφέρεται στο dump του χάρτη που θα μας παραδώσετε αλλιώς ο χάρτης που θα προκύψει θα είναι διαφορετικός και τα labels των clusters δεν θα αντιστοιχούν στην ανάλυσή σας. Οδηγίες για την αποθήκευση των dumps θα βρείτε στο [FAQ](https://docs.google.com/document/d/1-E4eQkVnTxa3Jb0HL9OAs11bugYRRZ7RNWpu7yh9G4s/edit?usp=sharing). \n",
    "- Μην ξεχάσετε στην αρχή ένα κελί markdown με **τα στοιχεία της ομάδας σας**.\n",
    "- Στο **zip** που θα παραδώσετε πρέπει να βρίσκονται **4 αρχεία (το .ipynb και το .py του notebook καθώς και τα δύο .pkl)**\n",
    "- **Το maximum upload filesize στο mycourses είναι 29MB**, εάν το zip σας είναι μεγαλύτερο, δεν έχετε βελτιστοποιήσει το tfidf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHhCkvxjnitd"
   },
   "source": [
    "<table>\n",
    "  <tr><td align=\"center\">\n",
    "    <font size=\"4\">Παρακαλούμε διατρέξτε βήμα-βήμα το notebook για να μην ξεχάσετε παραδοτέα!</font>\n",
    "</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a-FvmQai5S9m"
   },
   "source": [
    "## Ημερομηνία παράδοσης ##\n",
    "\n",
    "Κυριακή 19 Ιανουαρίου 2020.\n",
    "\n",
    "Καλή επιτυχία!\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Άσκηση 2 Εκφώνηση.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
